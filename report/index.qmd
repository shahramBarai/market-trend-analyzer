---
format:
  acm-pdf:
    include-in-header: header.tex

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
# keep-tex: true

bibliography: bibliography.bib

title: Market Trend Analyzer

author:
  - name: Yerzhan Zhamashev
    note: Both authors contributed equally to this work.
    email: yerzhan.zhamashev@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland
  - name: Shahram Barai*
    email: shahram.barai@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: sigconf

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: rightsretained

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
        <concept_id>10002951.10002952</concept_id>
        <concept_desc>Information systems~Data management systems</concept_desc>
        <concept_significance>500</concept_significance>
        </concept>
      <concept>
        <concept_id>10011007.10011074</concept_id>
        <concept_desc>Software and its engineering~Software creation and management</concept_desc>
        <concept_significance>300</concept_significance>
        </concept>
      <concept>
        <concept_id>10010405.10010406</concept_id>
        <concept_desc>Applied computing~Enterprise computing</concept_desc>
        <concept_significance>300</concept_significance>
      </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[500]{Information systems~Data management systems}
    \ccsdesc[300]{Software and its engineering~Software creation and management}
    \ccsdesc[300]{Applied computing~Enterprise computing}

  keywords:
    - data analysis
    - data visualization
    - real-time data
    - financial data
    - decision support
    - system performance

abstract: |
  This is a course project report for the course "CS-E4780 - Scalable Systems and Data Management D" at Aalto University. The project addresses the analysis of financial tick data to detect trading trends by implementing a system capable of identifying price movement patterns for individual symbols and generating actionable buy/sell advisories. Using a real-world dataset comprising one week of high-frequency market events across multiple European exchanges, two primary analyses were conducted: (1) calculating Exponential Moving Averages (EMAs) over fixed time windows to detect short-term price trends and (2) identifying bullish and bearish breakout patterns to generate buy and sell advisories, respectively. The developed system also includes a visualization component to highlight detected patterns, enhancing trader decision-making capabilities. Evaluation focuses on performance, scalability, and accuracy in processing high-volume event streams.
---

# Introduction

The detection of trading trends in financial markets is a critical task for analysts and traders. With the rapid growth of market data, the need for systems capable of processing and analyzing high-frequency data in real time has become more pressing. This project, performed as part of the “CS-E4780 - Scalable Systems and Data Management D” course at Aalto University, aims to address this challenge by developing a system that detects trading trends and provides actionable buy/sell advisories.

The system leverages a real-world dataset consisting of one week’s worth of financial tick data collected from multiple European exchanges. The project implements two primary analyses: calculating Exponential Moving Averages (EMAs) to detect price trends and identifying bullish and bearish breakout patterns to generate buy and sell advisories. The system also includes a visualization component to display the detected trends and advisories, although it is still under development.

The architecture of the system is built to handle real-time data streams. The first component is the real-time data producer, which emulates a real-time stream of financial data by preprocessing a large CSV file and splitting events by event ID (share symbol + region) using a Python script. This data is then passed to a series of worker processes written in Rust with the Tokio library, which synchronizes and sends the data to Kafka topics based on the event region. Kafka acts as the buffer for this event data, ensuring scalability and reliability.

For real-time analytics, the project uses Flink to process events in 5-minute windows, with each task manager subscribing to a specific Kafka topic (representing a region) to handle relevant data. The output of this analysis is sent back to the Kafka analytics topic, which the client can subscribe to to access the processed results. This data informs the detection of price trends and breakout patterns, leading to actionable buy and sell advisories.

Web dashboard for visializing the result....

Kubernetes for auto-scaling the system ....

## Data Set

The data used for this course project are based on a week’s worth of real tick data captured by Infront Financial Technology GmbH in 2021. The full data set Trading Data used for the project is publicly available from @frischbier_2022_6382482 licensed under an open license[^1].

[^1]: [http://creativecommons.org/licenses/by-nc-sa/4.0/](http://creativecommons.org/licenses/by-nc-sa/4.0/)

# System Architecture

The architecture of the Market Trend Analyzer is designed with a primary focus on scalability, real-time processing, and efficient handling of high-volume financial data streams. The system is composed of several components, each chosen and designed to meet specific requirements that allow for efficient processing, analysis, and delivery of market insights.

<!-- Import image -->
<!-- ![System Architecture](images/demo_img.png) -->


## Real-time Data Producer
The first design choice is the real-time data producer, which emulates the continuous flow of market data by preprocessing a large CSV file. This approach ensures that the system can handle high-frequency data streams and provides a realistic simulation of real-time market events. The decision to split the data by event ID (comprising the share symbol and region) allows the system to organize and process data efficiently, ensuring that each event is correctly attributed to its corresponding symbol. The use of Rust with the Tokio library for the worker processes enables high-performance, asynchronous processing of data, which is essential for handling large volumes of events in real-time. The choice of Rust was driven by its strong performance characteristics, memory safety, and concurrency support, making it well-suited for building high-performance, real-time systems.

## Data pre-processing and load balancing
The data pre-processing and load balancing components are responsible for "subcribing" to the data producer, preprocessing the data (e.g. filtering, transforming, and partitioning), and distributing the data to the appropriate Kafka topics. This component currently for prototyping reasons is implemented with the real-time data producer, but in a production environment, it would be a separate component.

## Kafka for Data Buffering
Apache Kafka is used as the data buffer to store and manage the high-volume event data generated by the real-time data producer. Kafka provides a scalable, fault-tolerant, and distributed messaging system that can handle large amounts of data efficiently. By using Kafka, the system can ensure that data is reliably stored and processed, even in the event of failures or network issues. The decision to partition the data by region in Kafka aligns with the need to process events based on their geographical origin, enabling efficient data processing and analysis. Kafka's ability to handle high-throughput data streams and provide real-time data processing capabilities makes it an ideal choice for the system's architecture. But the decision to devide the data by region is based on the limited resources. In real world, the data should be divided by the symbol of the share. This way, consumers can subscribe to the data (i.e. Kafka topics) based on the share symbol they are interested in. This allows for more efficient data processing and analysis, as consumers only receive the data they need, and the system can be scaled horizontally by create new topics for new shares.

## Flink for Real-time Analytics
Apache Flink is used for real-time analytics to process the high-frequency market events in 5-minute windows. Flink provides a powerful stream processing framework that supports event-time processing, stateful computations, and fault tolerance. By using Flink, the system can perform complex analytics on the event data, such as calculating Exponential Moving Averages (EMAs) and identifying bullish and bearish breakout patterns. The decision to use Flink for real-time analytics was driven by its ability to handle high-throughput data streams, provide low-latency processing, and support stateful computations. The use of Flink allows the system to process the event data in near real-time, enabling the detection of price trends and breakout patterns as they occur. The output of the Flink analytics is sent back to Kafka, where it can be consumed by clients to access the processed results.

## ?? Web Dashboard (Under Development)
...

## ?? Kubernetes for Auto-Scaling
...

# Implementation

Explain how the project was implemented, including the hardware/cloud services, programming tools,and GUI tools used

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Plot a generic plot
plt.plot(np.random.randn(1000).cumsum())
```

# Performance Evaluation

Assess the system’s performance, including its correctness (e.g., whether query results are accurate), scalability (how the system performs with varying resources, such as CPU, or workloads, such as events per second), and resource utilization (e.g., GPU and CPU usage).

# Conclusion

Summarize the project and its contributions, and discuss potential future work.

# References
