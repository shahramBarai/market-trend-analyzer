---
format:
  acm-pdf:
    include-in-header: header.tex

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
# keep-tex: true

bibliography: bibliography.bib

title: Market Trend Analyzer

author:
  - name: Yerzhan Zhamashev
    note: Both authors contributed equally to this work.
    email: yerzhan.zhamashev@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland
  - name: Shahram Barai*
    email: shahram.barai@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: sigconf

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: rightsretained

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
        <concept_id>10002951.10002952</concept_id>
        <concept_desc>Information systems~Data management systems</concept_desc>
        <concept_significance>500</concept_significance>
        </concept>
      <concept>
        <concept_id>10011007.10011074</concept_id>
        <concept_desc>Software and its engineering~Software creation and management</concept_desc>
        <concept_significance>300</concept_significance>
        </concept>
      <concept>
        <concept_id>10010405.10010406</concept_id>
        <concept_desc>Applied computing~Enterprise computing</concept_desc>
        <concept_significance>300</concept_significance>
      </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[500]{Information systems~Data management systems}
    \ccsdesc[300]{Software and its engineering~Software creation and management}
    \ccsdesc[300]{Applied computing~Enterprise computing}

  keywords:
    - data analysis
    - data visualization
    - real-time data
    - financial data
    - decision support
    - system performance

abstract: |
  This is a course project report for the course "CS-E4780 - Scalable Systems and Data Management D" at Aalto University. The project addresses the analysis of financial tick data to detect trading trends by implementing a system capable of identifying price movement patterns for individual symbols and generating actionable buy/sell advisories. Using a real-world dataset comprising one week of high-frequency market events across multiple European exchanges, two primary analyses were conducted: (1) calculating Exponential Moving Averages (EMAs) over fixed time windows to detect short-term price trends and (2) identifying bullish and bearish breakout patterns to generate buy and sell advisories, respectively. The developed system also includes a visualization component to highlight detected patterns, enhancing trader decision-making capabilities. Evaluation focuses on performance, scalability, and accuracy in processing high-volume event streams.
---

# Introduction

The detection of trading trends in financial markets is a critical task for analysts and traders. With the rapid growth of market data, the need for systems capable of processing and analyzing high-frequency data in real time has become more pressing. This project, performed as part of the “CS-E4780 - Scalable Systems and Data Management D” course at Aalto University, aims to address this challenge by developing a system that detects trading trends and provides actionable buy/sell advisories.

The system leverages a real-world dataset consisting of one week’s worth of financial tick data collected from multiple European exchanges. The project implements two primary analyses: calculating Exponential Moving Averages (EMAs) to detect price trends and identifying bullish and bearish breakout patterns to generate buy and sell advisories. The system also includes a visualization component to display the detected trends and advisories, although it is still under development.

The architecture of the system is built to handle real-time data streams. The first component is the real-time data producer, which emulates a real-time stream of financial data by preprocessing a large CSV file and splitting events by event ID (share symbol + region) using a Python script. This data is then passed to a series of worker processes written in Rust with the Tokio library, which synchronizes and sends the data to Kafka topics based on the event region. Kafka acts as the buffer for this event data, ensuring scalability and reliability.

For real-time analytics, the project uses Flink to process events in 5-minute windows, with each task manager subscribing to a specific Kafka topic (representing a region) to handle relevant data. The output of this analysis is sent back to the Kafka analytics topic, which the client can subscribe to to access the processed results. This data informs the detection of price trends and breakout patterns, leading to actionable buy and sell advisories.

Web dashboard for visializing the result....

Kubernetes for auto-scaling the system ....

## Data Set

The data used for this course project are based on a week’s worth of real tick data captured by Infront Financial Technology GmbH in 2021. The full data set Trading Data used for the project is publicly available from @frischbier_2022_6382482 licensed under an open license[^1].

[^1]: [http://creativecommons.org/licenses/by-nc-sa/4.0/](http://creativecommons.org/licenses/by-nc-sa/4.0/)

# System Architecture

The Market Trend Analyzer’s architecture is designed with a focus on scalability, real-time processing, and efficient handling of high-volume financial data streams. It consists of multiple components, each selected and implemented to meet specific requirements for efficient data ingestion, processing, analysis, and delivery of market insights. The following diagram illustrates the system architecture and the data flow between these components.

![Context view](images/context_view.jpg)

We use Docker and Docker Compose to containerize and orchestrate all system components. Containerization simplifies deployment, ensuring the system runs consistently across different environments—be it a local machine, cloud provider, or Kubernetes cluster. The decision to use Docker and Docker Compose was driven by the need for a straightforward deployment process and environment consistency.

The system is divided into several services, each representing a different component: the real-time data producer, Kafka, Flink, TimescaleDB, and the web application. Each service has its own Dockerfile defining base images, dependencies, and configurations. Docker Compose then orchestrates these services, specifying their relationships and dependencies, which simplifies the overall setup and management.

## Producer for Real-time Data Generation
The producer component is split into two parts: subscription to a data source and a producer module. The subscription part fetches real-time market data, while the producer module processes and routes this data to the appropriate Kafka topics. This design ensures that the system can handle high-frequency, real-world events rather than relying on pre-fetched or batch data. The goal is to simulate real-time data streams and validate the system’s ability to handle large volumes of events effectively.

Both parts of the producer are implemented in Rust, using the Tokio library for asynchronous processing. Rust was chosen for its high performance, memory safety, and concurrency support. Tokio’s asynchronous runtime allows the system to handle multiple concurrent connections and non-blocking I/O efficiently. Together, Rust and Tokio enable the system to process data streams in real time and manage high-frequency events without performance bottlenecks.

![Producer service](images/producer_service.jpg)

### Subscription to a data source
We first split a large CSV file into smaller files by share symbol and region using a Python script. Python was chosen for this initial data processing step due to its ease of use and flexibility. Because this step is only performed once before the system runs, Python’s performance overhead is not critical.

After splitting the data, we use Rust (with Tokio) to create a pool of worker tasks that read the CSV data and send it to the producer module according to the original trading times. This approach allows us to simulate real-time data ingestion by reproducing the original event order and timing, ensuring that the system genuinely processes events in a streaming manner.

### Producer features
The producer module subscribes to the data source and routes the incoming events to the correct Kafka topics. We chose to implement the producer in Rust as well, maintaining language consistency with the subscription workers and simplifying integration and code reuse.

An alternative considered was Elixir, known for its suitability in distributed, fault-tolerant, and highly concurrent systems. Elixir provides lightweight concurrency and built-in fault tolerance, making it an attractive option for real-time data processing. However, we opted for Rust to keep the system architecture simpler and reduce the number of technologies. By using Rust end-to-end in the producer subsystem, we ensure that high-frequency data streams are handled efficiently and reliably.

Another key design decision was to use Protobuf for data serialization before sending messages to Kafka. Protobuf’s binary serialization format is highly efficient in both size and speed, outperforming JSON or XML for high-throughput data systems. This choice reduces network overhead and improves overall performance.

In addition to Protobuf, we apply Snappy compression to the serialized data before sending it to Kafka. Snappy offers excellent compression and decompression speeds, making it a great fit for a high-throughput, low-latency system. Compared to other algorithms like Gzip, Zstd, or LZ4, Snappy strikes a good balance between compression ratio and speed, further optimizing performance and reducing network bandwidth usage.

## Kafka for Data Buffer
Apache Kafka is used as the data buffer to store and manage the high-volume event data generated by the real-time data producer. Kafka provides a scalable, fault-tolerant, and distributed messaging system that can handle large amounts of data efficiently. By using Kafka, the system can ensure that data is reliably stored and processed, even in the event of failures or network issues. The decision to partition the data by region in Kafka aligns with the need to process events based on their geographical origin, enabling efficient data processing and analysis. Kafka's ability to handle high-throughput data streams and provide real-time data processing capabilities makes it an ideal choice for the system's architecture. 

The decision to devide the data by region is based on the limited resources. In real world, the data should be divided by the symbol of the share. This way, consumers can subscribe to the data (i.e. Kafka topics) based on the share symbol they are interested in. This allows for more efficient data processing and analysis, as consumers only receive the data they need, and the system can be scaled horizontally by create new topics for new shares.

Splitting the data by region is a good compromise for the project, as it allows us to demonstrate the system's capabilities without the need for a large number of Kafka topics. In our project, each region has its own Kafka topics for the tick data, EMA results, and advisories (buy/sell signals). This design choice allows for efficient data processing and analysis, as each region's data can be processed independently and in parallel. The use of Kafka topics for each region ensures that the system can scale horizontally by adding more Kafka brokers and partitions as needed, enabling the system to handle large volumes of data and provide low-latency processing.

## Flink for Real-time Analytics
Apache Flink is used for real-time analytics to process the high-frequency market events in 5-minute windows. The decision to use Flink for real-time analytics was driven by its ability to handle high-throughput data streams, provide low-latency processing, and support stateful computations. This is allows our system to process the event data in near real-time, enabling the EMA calculations and breakout pattern detection to be performed efficiently and accurately. Flink's support for event time processing and windowing allows the system to process the data based on the event timestamps, ensuring that the analysis is performed correctly and in the correct order.

Alternative to Flink we also considered using Apache Spark, which is a popular distributed data processing framework that provides similar capabilities for real-time analytics. But we decided to use Flink instead of Spark because of Flink's better support for event time processing and windowing, which are essential for processing high-frequency event data. Also beased on the performance comparison, Flink is more efficient in processing high-frequency event data compared to Spark wich is more suitable for batch processing.

## TimescaleDB for Data Storage
TimescaleDB stores processed data such as EMA results and advisories. As a time-series database, TimescaleDB is optimized for querying time-indexed data, making it an ideal choice for storing historical EMAs and advisories. This setup allows the system to provide historical insights and improve user experience. Although we currently use a single TimescaleDB instance, the system can easily scale horizontally by adding more instances behind a load balancer, enabling efficient handling of large datasets and low-latency queries.

## Web application
The web application provides a user-friendly interface for visualizing trends and advisories. We built it with React for UI flexibility and Tailwind CSS for rapid styling, benefiting from their large communities and rich ecosystems.

The web application includes charts for EMA visualization, tables for buy/sell advisories, and components for the latest share prices. Communication between the frontend and backend uses Socket.IO for real-time, bidirectional event streaming. The server subscribes to Kafka topics to receive processed data from Flink and tick data from the producer, then pushes this information to the client via Socket.IO for immediate display.

While we currently run a single instance of the web application, it can be horizontally scaled by adding more server instances. This would be straightforwardly automated using Kubernetes, simplifying deployment, scaling, and management. Given the project’s scope, we maintain a single instance for the demonstration, focusing on core functionality rather than large-scale scaling of the web layer.

# Implementation

Explain how the project was implemented, including the hardware/cloud services, programming tools,and GUI tools used

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Plot a generic plot
plt.plot(np.random.randn(1000).cumsum())
```

# Performance Evaluation

Assess the system’s performance, including its correctness (e.g., whether query results are accurate), scalability (how the system performs with varying resources, such as CPU, or workloads, such as events per second), and resource utilization (e.g., GPU and CPU usage).

# Conclusion

Summarize the project and its contributions, and discuss potential future work.

# References
