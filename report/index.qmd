---
format:
  acm-pdf:
    include-in-header: header.tex

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
# keep-tex: true

bibliography: bibliography.bib

title: Market Trend Analyzer

author:
  - name: Yerzhan Zhamashev
    note: Both authors contributed equally to this work.
    email: yerzhan.zhamashev@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland
  - name: Shahram Barai*
    email: shahram.barai@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: sigconf

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: rightsretained

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
        <concept_id>10002951.10002952</concept_id>
        <concept_desc>Information systems~Data management systems</concept_desc>
        <concept_significance>500</concept_significance>
        </concept>
      <concept>
        <concept_id>10011007.10011074</concept_id>
        <concept_desc>Software and its engineering~Software creation and management</concept_desc>
        <concept_significance>300</concept_significance>
        </concept>
      <concept>
        <concept_id>10010405.10010406</concept_id>
        <concept_desc>Applied computing~Enterprise computing</concept_desc>
        <concept_significance>300</concept_significance>
      </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[500]{Information systems~Data management systems}
    \ccsdesc[300]{Software and its engineering~Software creation and management}
    \ccsdesc[300]{Applied computing~Enterprise computing}

  keywords:
    - data analysis
    - data visualization
    - real-time data
    - financial data
    - decision support
    - system performance

abstract: |
  This is a course project report for the course "CS-E4780 - Scalable Systems and Data Management D" at Aalto University. The project addresses the analysis of financial tick data to detect trading trends by implementing a system capable of identifying price movement patterns for individual symbols and generating actionable buy/sell advisories. Using a real-world dataset comprising one week of high-frequency market events across multiple European exchanges, two primary analyses were conducted: (1) calculating Exponential Moving Averages (EMAs) over fixed time windows to detect short-term price trends and (2) identifying bullish and bearish breakout patterns to generate buy and sell advisories, respectively. The developed system also includes a visualization component to highlight detected patterns, enhancing trader decision-making capabilities.
---

# Introduction

The Market Trend Analyzer is a comprehensive system designed to handle high-frequency financial data streams, providing real-time analytics and actionable insights. Leveraging a publicly available dataset of trading events captured by Infront Financial Technology GmbH in 2021, this project demonstrates the integration of modern data processing, messaging, and storage technologies to build a robust and scalable analytics pipeline.

The architecture of the Market Trend Analyzer focuses on scalability and real-time processing. By employing containerized microservices, the system ensures consistent and portable deployments across diverse environments, from local machines to cloud platforms. Each component—data producer, Kafka messaging system, Flink analytics engine, TimescaleDB storage, and the web application—plays a critical role in transforming raw trading data into meaningful insights.

The project showcases state-of-the-art technologies like Apache Kafka for fault-tolerant data buffering, Apache Flink for real-time analytics, and TimescaleDB for optimized storage of time-series data. Through a Python-based preprocessing step and a Rust-implemented real-time data producer, historical tick data is seamlessly converted into simulated live streams, ensuring authenticity and temporal accuracy in downstream computations.

By integrating these components, the Market Trend Analyzer illustrates how high-performance streaming systems can be built for financial applications, paving the way for advanced analytics and decision-making in dynamic market environments. This report details the implementation, architecture, and evaluation of the system, highlighting its design choices and technological innovations.

## Data Set

The data used for this course project are based on a week’s worth of real tick data captured by Infront Financial Technology GmbH in 2021. The full data set Trading Data used for the project is publicly available from @frischbier_2022_6382482 licensed under an open license[^1].

[^1]: [http://creativecommons.org/licenses/by-nc-sa/4.0/](http://creativecommons.org/licenses/by-nc-sa/4.0/)

# System Architecture

The Market Trend Analyzer’s architecture is designed with a focus on scalability, real-time processing, and efficient handling of high-volume financial data streams. It consists of multiple components, each selected and implemented to meet specific requirements for efficient data ingestion, processing, analysis, and delivery of market insights. 

We use Docker and Docker Compose to containerize and orchestrate all system components. Containerization simplifies deployment, ensuring the system runs consistently across different environments—be it a local machine, cloud provider, or Kubernetes cluster. The decision to use Docker and Docker Compose was driven by the need for a straightforward deployment process and environment consistency.

The system is divided into several services, each representing a different component: the real-time data producer, Kafka, Flink, TimescaleDB, and the web application. Each service has its own Dockerfile defining base images, dependencies, and configurations. Docker Compose then orchestrates these services, specifying their relationships and dependencies, which simplifies the overall setup and management.

The following diagram illustrates the system architecture and the data flow between these components.

![Context view](images/context_view.jpg)

## Producer for Real-time Data Generation
The producer component is split into two parts: subscription to a data source and a producer module. The subscription part fetches real-time market data, while the producer module processes and routes this data to the appropriate Kafka topics. This design ensures that the system can handle high-frequency, real-world events rather than relying on pre-fetched or batch data. The goal is to simulate real-time data streams and validate the system’s ability to handle large volumes of events effectively.

Both parts of the producer are implemented in Rust, using the Tokio library for asynchronous processing. Rust was chosen for its high performance, memory safety, and concurrency support. Tokio’s asynchronous runtime allows the system to handle multiple concurrent connections and non-blocking I/O efficiently. Together, Rust and Tokio enable the system to process data streams in real time and manage high-frequency events without performance bottlenecks.

![Producer service](images/producer_service.jpg)

### Subscription to a data source
We first split a large CSV file into smaller files by share symbol and region using a Python script. Python was chosen for this initial data processing step due to its ease of use and flexibility. Because this step is only performed once before the system runs, Python’s performance overhead is not critical.

After splitting the data, we use Rust (with Tokio) to create a pool of worker tasks that read the CSV data and send it to the producer module according to the original trading times. This approach allows us to simulate real-time data ingestion by reproducing the original event order and timing, ensuring that the system genuinely processes events in a streaming manner.

### Producer features
The producer module subscribes to the data source and routes the incoming events to the correct Kafka topics. We chose to implement the producer in Rust as well, maintaining language consistency with the subscription workers and simplifying integration and code reuse.

An alternative considered was Elixir, known for its suitability in distributed, fault-tolerant, and highly concurrent systems. Elixir provides lightweight concurrency and built-in fault tolerance, making it an attractive option for real-time data processing. However, we opted for Rust to keep the system architecture simpler and reduce the number of technologies. By using Rust end-to-end in the producer subsystem, we ensure that high-frequency data streams are handled efficiently and reliably.

Another key design decision was to use Protobuf for data serialization before sending messages to Kafka. Protobuf’s binary serialization format is highly efficient in both size and speed, outperforming JSON or XML for high-throughput data systems. This choice reduces network overhead and improves overall performance.

In addition to Protobuf, we apply Snappy compression to the serialized data before sending it to Kafka. Snappy offers excellent compression and decompression speeds, making it a great fit for a high-throughput, low-latency system. Compared to other algorithms like Gzip, Zstd, or LZ4, Snappy strikes a good balance between compression ratio and speed, further optimizing performance and reducing network bandwidth usage.

## Kafka for Data Buffer
Apache Kafka is used as the data buffer to store and manage the high-volume event data generated by the real-time data producer. Kafka provides a scalable, fault-tolerant, and distributed messaging system that can handle large amounts of data efficiently. By using Kafka, the system can ensure that data is reliably stored and processed, even in the event of failures or network issues. The decision to partition the data by region in Kafka aligns with the need to process events based on their geographical origin, enabling efficient data processing and analysis. Kafka's ability to handle high-throughput data streams and provide real-time data processing capabilities makes it an ideal choice for the system's architecture. 

The decision to devide the data by region is based on the limited resources. In real world, the data should be divided by the symbol of the share. This way, consumers can subscribe to the data (i.e. Kafka topics) based on the share symbol they are interested in. This allows for more efficient data processing and analysis, as consumers only receive the data they need, and the system can be scaled horizontally by create new topics for new shares.

Splitting the data by region is a good compromise for the project, as it allows us to demonstrate the system's capabilities without the need for a large number of Kafka topics. In our project, each region has its own Kafka topics for the tick data, EMA results, and advisories (buy/sell signals). This design choice allows for efficient data processing and analysis, as each region's data can be processed independently and in parallel. The use of Kafka topics for each region ensures that the system can scale horizontally by adding more Kafka brokers and partitions as needed, enabling the system to handle large volumes of data and provide low-latency processing.

## Flink for Real-time Analytics
Apache Flink is used for real-time analytics to process the high-frequency market events in 5-minute windows. The decision to use Flink for real-time analytics was driven by its ability to handle high-throughput data streams, provide low-latency processing, and support stateful computations. This is allows our system to process the event data in near real-time, enabling the EMA calculations and breakout pattern detection to be performed efficiently and accurately. Flink's support for event time processing and windowing allows the system to process the data based on the event timestamps, ensuring that the analysis is performed correctly and in the correct order.

Alternative to Flink we also considered using Apache Spark, which is a popular distributed data processing framework that provides similar capabilities for real-time analytics. But we decided to use Flink instead of Spark because of Flink's better support for event time processing and windowing, which are essential for processing high-frequency event data. Also beased on the performance comparison, Flink is more efficient in processing high-frequency event data compared to Spark wich is more suitable for batch processing.

## TimescaleDB for Data Storage
TimescaleDB stores processed data such as EMA results and advisories. As a time-series database, TimescaleDB is optimized for querying time-indexed data, making it an ideal choice for storing historical EMAs and advisories. This setup allows the system to provide historical insights and improve user experience. Although we currently use a single TimescaleDB instance, the system can easily scale horizontally by adding more instances behind a load balancer, enabling efficient handling of large datasets and low-latency queries.

## Web application
The web application provides a user-friendly interface for visualizing trends and advisories. We built it with React for UI flexibility and Tailwind CSS for rapid styling, benefiting from their large communities and rich ecosystems.

The web application includes charts for EMA visualization, tables for buy/sell advisories, and components for the latest share prices. Communication between the frontend and backend uses Socket.IO for real-time, bidirectional event streaming. The server subscribes to Kafka topics to receive processed data from Flink and tick data from the producer, then pushes this information to the client via Socket.IO for immediate display.

While we currently run a single instance of the web application, it can be horizontally scaled by adding more server instances. This would be straightforwardly automated using Kubernetes, simplifying deployment, scaling, and management. Given the project’s scope, we maintain a single instance for the demonstration, focusing on core functionality rather than large-scale scaling of the web layer.

# Implementation

The Market Trend Analyzer was tested on a local Ubuntu 22.04.5 LTS machine (HP EliteBook 810 G10, 13th Gen Intel® Core™ i5-1335U, 32 GB RAM). This hardware provided ample resources to run multiple containerized services simultaneously, ensuring a stable development and testing environment.

### Containerization and Orchestration
All components — producer, messaging, analytics, storage, and web interface—were packaged as Docker containers. Docker Compose defined the relationships between these services, simplifying startup and shutdown processes. By using containers, the system could be easily deployed and kept consistent across different environments.

### Data Preprocessing with Python
A Python script ran once before the system started to prepare the input data. It read the original large CSV file of historical trading events and split it into smaller files organized by symbol and region. This pre-processing step ensured that the real-time producer had direct access to well-structured, symbol-specific data streams without runtime filtering overhead.

## The Real-time Data Producer
Once the preprocessing step is complete and the smaller CSV files are in place, the Rust-based producer component handles the real-time data ingestion and streaming to Kafka. The producer’s main workflow, as illustrated by the provided main.rs and csv.rs code, involves several key steps:

**Target Start Time Alignment:** The producer reads a target start time from a given "topic"-like string (e.g., "08-11-2021 10:00:00") and calculates a time offset relative to the current wall-clock time. This offset aligns the historical trading events with the present, effectively recreating the live market conditions at the chosen start time.

**Asynchronous Data Reading:** Using the Tokio runtime for asynchronous execution, the producer spawns multiple worker tasks (TradingFileProcessor) to handle each CSV file in parallel.
  - Preprocessing Phase: Each worker first scans through its CSV to skip any events that should have already occurred before the target start time. It then synchronizes with other workers via a barrier, ensuring all files begin streaming events at the correct, unified start time.
  - Real-time Simulation: Once synchronized, each worker reads future-dated events from its CSV. Before emitting each event, it calculates how long it should wait (sleep) until the event’s designated timestamp occurs in "real-time." This ensures that events are replayed chronologically and at the correct intervals, mirroring the timing of the original trading day.

**Data Serialization and Sending to Kafka:** Each financial event is encapsulated in a FinancialTick structure and serialized using Protobuf (prost crates) to produce a compact binary message. This reduces overhead and increases performance compared to text-based formats like JSON.

After serialization, the producer uses the rdkafka library to send messages to Kafka. The chosen Kafka topic naming convention incorporates the region (e.g., region-ticks), allowing for logical partitioning of data. By enabling Snappy compression in the Kafka producer configuration (compression.type=snappy), the system further optimizes bandwidth usage and latency.

**Concurrent Task Execution and Batching:** The producer leverages Tokio’s asynchronous tasks to handle I/O operations without blocking, spawning separate tasks for sending messages to Kafka. It also manages batching and flushing strategies: while events are processed as they come due, the system periodically flushes buffered output to ensure low latency while balancing performance.

Monitoring and Logging: Throughout execution, the producer logs its progress—when messages are sent, if any errors occur, and how many events are skipped or processed. This feedback loop assists in debugging and verifying that the simulation accurately reflects real-time conditions.

### Messaging and Data Buffering (Kafka)
Apache Kafka served as the central messaging hub, buffering high-volume event data from the producer. Topics were explicitly created and managed via a shell script and a JSON configuration file. Protobuf serialization and Snappy compression optimized network usage. This setup decoupled the producer from downstream consumers, ensuring scalability and reliable data delivery.

## Stream Processing and Analytics (Flink)
Flink serves as the core of the analytics pipeline, responsible for processing high-frequency financial data streams and generating actionable insights. Deployed as a JobManager and multiple TaskManagers, Flink is configured to connect seamlessly with Kafka, consuming Protobuf-encoded events in real-time. The system leverages event-time semantics and custom timestamp extraction to process each event based on its original trading time, ensuring precise temporal alignment and accurate computations.

Flink’s analytical workflow begins with event-time windowing, where incoming events are grouped into 5-minute tumbling windows based on their share symbol and timestamp. These windows facilitate the computation of Exponential Moving Averages (EMAs), with stateful EMACalculator functions updating and maintaining EMA values as new data flows in. This stateful design allows the system to provide continuous, real-time insights by iteratively refining the computed metrics.

Beyond EMA calculations, Flink employs pattern detection mechanisms to identify critical trading signals. Using a stateful CrossoverDetector, the system monitors interactions between short-term and long-term EMA values. When a short-term EMA surpasses or drops below a long-term EMA, Flink emits buy or sell advisories in near real-time. This pattern recognition transforms raw data streams into actionable intelligence, empowering decision-making in dynamic market conditions.

To maintain consistency and fault tolerance, Flink’s checkpointing mechanism is configured with EXACTLY_ONCE semantics. This setup ensures that even in the event of system failures, the pipeline can recover without data loss or duplication, meeting the stringent reliability requirements of financial analysis.

The processed outputs, including EMA results and trading advisories, are simultaneously written back to Kafka and stored in TimescaleDB. This dual-output approach ensures immediate availability for real-time dashboards while preserving historical data for retrospective analysis and long-term queries. This integration with both Kafka and TimescaleDB highlights Flink’s versatility in supporting both streaming and storage-oriented use cases within a unified system architecture.

## Storage and Querying (TimescaleDB)
TimescaleDB was deployed as a Docker container alongside the other services, with credentials and database name specified via environment variables. Upon startup, initialization scripts in ./configs/db-init executed SQL commands to configure TimescaleDB:

**Extension and Schema Setup:** The timescaledb extension was enabled to introduce time-series optimizations. Two tables, ema_results and buy_advisories, were created to store computed EMAs and trading advisories, respectively.

**Time-Series Partitioning:** Both tables were converted into hypertables using create_hypertable(). This command instructed TimescaleDB to partition the tables on the trade_timestamp column, ensuring that inserts and queries scale efficiently as data volumes grow.

Flink’s analytics pipeline wrote EMA results and advisories into TimescaleDB via JDBC sinks. By defining time-based primary keys and indexes, the database could efficiently store and retrieve historical market analytics, supporting both retrospective analysis and continuous, real-time updates.

### Web Application
The web application integrates a React-based front end with a Node.js/Express server, both running in same Docker container for simplicity and ease of deployment. Environment variables are used to configure the connection details for Kafka and TimescaleDB. Upon startup, the server leverages these variables to authenticate with TimescaleDB and to subscribe to the appropriate Kafka topics. The Node.js server employs Sequelize, a Node.js ORM, to interact with TimescaleDB and define models corresponding to the database tables. By using these models, the server can query historical EMA results and trading advisories without resorting to raw SQL commands.

The server runs as a Docker service, referencing a dedicated Dockerfile and mounting local source code, facilitating rapid development and deployment. When the container starts, the Node.js server connects to Kafka to consume live market event streams, processes these events, and delivers real-time updates to the React front end via Socket.IO. At the same time, requests for historical data are efficiently served through Sequelize queries against TimescaleDB, enabling the web interface to present both current conditions and archived records of computed indicators and advisories.

## Performance Evaluation

The Market Trend Analyzer was evaluated based on simulated real-time data streams, focusing on performance, scalability, and accuracy. The system handled each day’s worth of trading events in real-time with no backpressure, demonstrating its ability to process high-frequency data streams efficiently. Otherwise, each component can be reconfigured and scaled horizontally up to sequential processing of each symbol in parallel, leaving communication between the components as the main bottleneck.

# Conclusion

In conclusion, the Market Trend Analyzer successfully processes high-frequency financial data streams in near real-time, performs EMA calculations, detects breakout patterns, and generates actionable buy/sell advisories. Leveraging containerized microservices, it integrates a Rust-based real-time producer, Kafka for distributed messaging, Flink for analytics, and TimescaleDB for time-series data storage. The web application provides interactive visualizations and timely insights. This proof-of-concept system demonstrates the feasibility of building scalable, low-latency streaming pipelines for financial analytics, highlighting that further refinement, horizontal scaling, and more advanced analytical models can enhance its applicability and performance.

# References
