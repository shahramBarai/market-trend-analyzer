---
format:
  acm-pdf:
    include-in-header: header.tex

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS
# keep-tex: true

bibliography: bibliography.bib

title: Market Trend Analyzer

author:
  - name: Yerzhan Zhamashev
    note: Both authors contributed equally to this work.
    email: yerzhan.zhamashev@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland
  - name: Shahram Barai*
    email: shahram.barai@aalto.fi
    affiliation:
      name: Aalto University
      city: Espoo
      country: Finland

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  acmart-options: sigconf

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: rightsretained

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
        <concept_id>10002951.10002952</concept_id>
        <concept_desc>Information systems~Data management systems</concept_desc>
        <concept_significance>500</concept_significance>
        </concept>
      <concept>
        <concept_id>10011007.10011074</concept_id>
        <concept_desc>Software and its engineering~Software creation and management</concept_desc>
        <concept_significance>300</concept_significance>
        </concept>
      <concept>
        <concept_id>10010405.10010406</concept_id>
        <concept_desc>Applied computing~Enterprise computing</concept_desc>
        <concept_significance>300</concept_significance>
      </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[500]{Information systems~Data management systems}
    \ccsdesc[300]{Software and its engineering~Software creation and management}
    \ccsdesc[300]{Applied computing~Enterprise computing}

  keywords:
    - data analysis
    - data visualization
    - real-time data
    - financial data
    - decision support
    - system performance

abstract: |
  This is a course project report for the course "CS-E4780 - Scalable Systems and Data Management D" at Aalto University. The project addresses the analysis of financial tick data to detect trading trends by implementing a system capable of identifying price movement patterns for individual symbols and generating actionable buy/sell advisories. Using a real-world dataset comprising one week of high-frequency market events across multiple European exchanges, two primary analyses were conducted: (1) calculating Exponential Moving Averages (EMAs) over fixed time windows to detect short-term price trends and (2) identifying bullish and bearish breakout patterns to generate buy and sell advisories, respectively. The developed system also includes a visualization component to highlight detected patterns, enhancing trader decision-making capabilities. Evaluation focuses on performance, scalability, and accuracy in processing high-volume event streams.
---

# Introduction

The detection of trading trends in financial markets is a critical task for analysts and traders. With the rapid growth of market data, the need for systems capable of processing and analyzing high-frequency data in real time has become more pressing. This project, performed as part of the “CS-E4780 - Scalable Systems and Data Management D” course at Aalto University, aims to address this challenge by developing a system that detects trading trends and provides actionable buy/sell advisories.

The system leverages a real-world dataset consisting of one week’s worth of financial tick data collected from multiple European exchanges. The project implements two primary analyses: calculating Exponential Moving Averages (EMAs) to detect price trends and identifying bullish and bearish breakout patterns to generate buy and sell advisories. The system also includes a visualization component to display the detected trends and advisories, although it is still under development.

The architecture of the system is built to handle real-time data streams. The first component is the real-time data producer, which emulates a real-time stream of financial data by preprocessing a large CSV file and splitting events by event ID (share symbol + region) using a Python script. This data is then passed to a series of worker processes written in Rust with the Tokio library, which synchronizes and sends the data to Kafka topics based on the event region. Kafka acts as the buffer for this event data, ensuring scalability and reliability.

For real-time analytics, the project uses Flink to process events in 5-minute windows, with each task manager subscribing to a specific Kafka topic (representing a region) to handle relevant data. The output of this analysis is sent back to the Kafka analytics topic, which the client can subscribe to to access the processed results. This data informs the detection of price trends and breakout patterns, leading to actionable buy and sell advisories.

Web dashboard for visializing the result....

Kubernetes for auto-scaling the system ....

## Data Set

The data used for this course project are based on a week’s worth of real tick data captured by Infront Financial Technology GmbH in 2021. The full data set Trading Data used for the project is publicly available from @frischbier_2022_6382482 licensed under an open license[^1].

[^1]: [http://creativecommons.org/licenses/by-nc-sa/4.0/](http://creativecommons.org/licenses/by-nc-sa/4.0/)

# System Architecture

The architecture of the Market Trend Analyzer is designed with a primary focus on scalability, real-time processing, and efficient handling of high-volume financial data streams. The system is composed of several components, each chosen and designed to meet specific requirements that allow for efficient processing, analysis, and delivery of market insights. The following diagram illustrates the system architecture and the flow of data through the various components.

![Context view](images/context_view.jpg)

## Producer for Real-time Data Generation
The producer component is divided into two parts: the subscribtion to a data source and the producer features. The first part is responsible for generating real-time market data events and sending them to the producer. The second part is responsible for processing and routing the data to the appropriate Kafka topics. This desing choice was made to ensure that the system can handle high-frequency events in a real-world scenario, instead of relying on pre-fetched data to the system and processing it in a batch mode. We wanted to ensure that the system actually works with real-time data and can handle high-frequency events effectively.

Both parts of the producer component are implemented in Rust with the Tokio library. Rust was chosen for its high-performance, memory safety, and concurrency support, making it well-suited for building high-performance, real-time systems. The Tokio library provides asynchronous processing capabilities, allowing the system to handle multiple concurrent connections and perform non-blocking I/O operations efficiently. This design choice ensures that the system can process data streams in real-time and handle high-frequency events effectively.

![Producer service](images/producer_service.jpg)

### Subscription to a data source
To implement real-time data generation, we desided first to split and save large CSV file into smaller files based on the event ID (comprising the share symbol and region) by using a Python script. The decision to use Python for the spliting of the data was driven by the language's ease of use and flexibility in handling data processing tasks. Since we are only using Python for the data generation part, the performance overhead of Python is not a major concern. After splitting the data, we are using Rust with the Tokio library to create a workers pool that can read the data from the CSV files and send it to producer based on the trading time. This way we can simulate the real-time data generation by sending the data to the producer in the same order and time as it was recorded in the original CSV file. 

### Producer features
This component is responsible for subscribing to the data producer and routing the data to the appropriate Kafka topics. The decision to use Rust here was driven by keeping the same language for the producer and the worker processes, which allows for easier integration and code reuse. Alternative language to use for the producer instead of Rust is Elixir, which is a functional programming language that is well-suited for building distributed, fault-tolerant systems. Elixir provides a lightweight concurrency model and fault-tolerance mechanisms, making it an ideal choice for building real-time data processing systems. The decision to use Rust instead of Elixir was driven by the need to keep the system architecture simple and reduce the number of technologies used in the project. By using Rust for both the producer and the worker processes, we can ensure that the system can handle high-frequency data streams efficiently and reliably.

One of the key design choices in the producer component is that we decided to use protobuf for serializing the data before sending it to Kafka. Protobuf is a binary serialization format that is efficient in terms of both size and speed, making it well-suited for high-throughput data processing systems. By using protobuf, we can reduce the size of the data sent over the network and improve the system's performance. The decision to use protobuf instead of JSON or XML was driven by the need to optimize the system's performance and reduce the network bandwidth usage. 

Adittionally to use protobuf, we also decided to use snappy compression to compress the serialized data before sending it to Kafka, which also supports snappy compression and handles it for us. Compering to other compression algorithms, such as Gzip, Zstd, or LZ4, Snappy provides better performance in terms of compression speed and decompression speed, making it an ideal choice for the system's architecture. The decision to use snappy compression was driven by the need to reduce the network bandwidth usage and improve the system's performance by reducing the amount of data sent over the network.

## Kafka for Data Buffer
Apache Kafka is used as the data buffer to store and manage the high-volume event data generated by the real-time data producer. Kafka provides a scalable, fault-tolerant, and distributed messaging system that can handle large amounts of data efficiently. By using Kafka, the system can ensure that data is reliably stored and processed, even in the event of failures or network issues. The decision to partition the data by region in Kafka aligns with the need to process events based on their geographical origin, enabling efficient data processing and analysis. Kafka's ability to handle high-throughput data streams and provide real-time data processing capabilities makes it an ideal choice for the system's architecture. 

The decision to devide the data by region is based on the limited resources. In real world, the data should be divided by the symbol of the share. This way, consumers can subscribe to the data (i.e. Kafka topics) based on the share symbol they are interested in. This allows for more efficient data processing and analysis, as consumers only receive the data they need, and the system can be scaled horizontally by create new topics for new shares.

Splitting the data by region is a good compromise for the project, as it allows us to demonstrate the system's capabilities without the need for a large number of Kafka topics. In our project, each region has its own Kafka topics for the tick data, EMA results, and advisories (buy/sell signals). This design choice allows for efficient data processing and analysis, as each region's data can be processed independently and in parallel. The use of Kafka topics for each region ensures that the system can scale horizontally by adding more Kafka brokers and partitions as needed, enabling the system to handle large volumes of data and provide low-latency processing.

## Flink for Real-time Analytics
Apache Flink is used for real-time analytics to process the high-frequency market events in 5-minute windows. The decision to use Flink for real-time analytics was driven by its ability to handle high-throughput data streams, provide low-latency processing, and support stateful computations. This is allows our system to process the event data in near real-time, enabling the EMA calculations and breakout pattern detection to be performed efficiently and accurately. Flink's support for event time processing and windowing allows the system to process the data based on the event timestamps, ensuring that the analysis is performed correctly and in the correct order.

Alternative to Flink we also considered using Apache Spark, which is a popular distributed data processing framework that provides similar capabilities for real-time analytics. But we decided to use Flink instead of Spark because of Flink's better support for event time processing and windowing, which are essential for processing high-frequency event data. Also beased on the performance comparison, Flink is more efficient in processing high-frequency event data compared to Spark wich is more suitable for batch processing.

## TimescaleDB for Data Storage
...

## Web application

The web application is designed to provide a user-friendly interface to visualize the detected trends and advisories. The web application is built using React, a popular JavaScript library for building user interfaces, and Tailwind CSS, a utility-first CSS framework for styling web applications. The decision to use React and Tailwind CSS was driven by their ease of use, flexibility, and extensive community support, making them ideal choices for building modern web applications.

The web application consists of several components, including a chart component to visualize EMA result, a table component to show the buy and sell advisories, and a component to display the last share price. The connection between client and server is established using Socket.IO, a real-time bidirectional event-based communication library for web applications. Server subscribes to the Kafka topics to receive the processed data from Flink and ticks data from the producer. The server then sends this data to the client using Socket.IO, allowing the client to receive real-time updates and display the data.

The web application service is designed so it can be easily scaled horizontally by adding more instances of the server. This allows the system to handle a large number of concurrent users and provide low-latency updates to the client. Handeling the scaling of the web application currently is done manually, by changing replication factor in the Docker Compose file. But this can be automated by using Kubernetes, which provides a container orchestration platform for automating the deployment, scaling, and management of containerized applications.

But for semplicity and the reson that the scaling of the web application is not the main focus of the project, we decided to use only one instance for the server and the client.

# Implementation

Explain how the project was implemented, including the hardware/cloud services, programming tools,and GUI tools used

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

# Plot a generic plot
plt.plot(np.random.randn(1000).cumsum())
```

# Performance Evaluation

Assess the system’s performance, including its correctness (e.g., whether query results are accurate), scalability (how the system performs with varying resources, such as CPU, or workloads, such as events per second), and resource utilization (e.g., GPU and CPU usage).

# Conclusion

Summarize the project and its contributions, and discuss potential future work.

# References
